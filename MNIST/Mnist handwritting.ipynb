{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime      # datetime.now() 를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # 은닉층 가중치  W2 = (784 X 100) Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3 = (100X10)  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "        \n",
    "        \n",
    "        #Xavier/He 방법\n",
    "        #https://gomguard.tistory.com/183\n",
    "        #https://reniew.github.io/13/\n",
    "                        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의 (모두 행렬로 표시)\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의 (모두 행렬로 표시)\n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 정의 (모두 행렬로 표시)\n",
    "        self.Z1 = np.zeros([1,input_nodes])    \n",
    "        self.A1 = np.zeros([1,input_nodes])       \n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "        \n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산    \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )\n",
    "    \n",
    "   \n",
    "    # 정확도 측정함수\n",
    "    def accuracy(self, test_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        for index in range(len(test_data)):\n",
    "                        \n",
    "            label = int(test_data[index, 0])\n",
    "                        \n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\n",
    "            data = (test_data[index, 1:] / 255.0 * 0.99) + 0.01\n",
    "            \n",
    "                  \n",
    "            # predict 를 위해서 vector 을 matrix 로 변환하여 인수로 넘겨줌\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "        print(\"Current Accuracy = \", 100*(len(matched_list)/(len(test_data))), \" %\")\n",
    "        \n",
    "        not_matched_list=not_matched_list\n",
    "        \n",
    "        return not_matched_list\n",
    "    \n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 개, target_data : 10개\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # 먼저 feed forward 를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        # 출력층 loss 인 loss_3 구함\n",
    "        loss_3 = (self.A3-self.target_data) * self.A3 * (1-self.A3)\n",
    "                        \n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate * np.dot(self.A2.T, loss_3)   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3\n",
    "        \n",
    "        # 은닉층 loss 인 loss_2 구함        \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate * np.dot(self.A1.T, loss_2)   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "        \n",
    "    def predict(self, input_data):        # input_data 는 행렬로 입력됨 즉, (1, 784) shape 을 가짐        \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        predicted_num = np.argmax(A3)\n",
    "    \n",
    "        return predicted_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 training data 읽어옴\n",
    "training_data = np.loadtxt('../../data/classic_mnist/mnist_train.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "# 0~9 숫자 이미지가 784개의 숫자 (28X28) 로 구성되어 있는 test data 읽어옴\n",
    "test_data = np.loadtxt('../../data/classic_mnist/mnist_test.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print (training_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 ,  loss_val =  3.7319855101362314\n",
      "step =  1000 ,  loss_val =  0.8259338707761567\n",
      "step =  2000 ,  loss_val =  1.3178724027351796\n",
      "step =  3000 ,  loss_val =  2.2079998144805377\n",
      "step =  4000 ,  loss_val =  0.9435255113098362\n",
      "step =  5000 ,  loss_val =  0.7546457050075105\n",
      "step =  6000 ,  loss_val =  0.8386522043317712\n",
      "step =  7000 ,  loss_val =  2.59409834749247\n",
      "step =  8000 ,  loss_val =  1.019946050327303\n",
      "step =  9000 ,  loss_val =  0.9769363805804416\n",
      "step =  10000 ,  loss_val =  0.9874268096182706\n",
      "step =  11000 ,  loss_val =  0.9351886597683985\n",
      "step =  12000 ,  loss_val =  0.9045028678298771\n",
      "step =  13000 ,  loss_val =  0.9136063589457947\n",
      "step =  14000 ,  loss_val =  0.9843355611478356\n",
      "step =  15000 ,  loss_val =  0.9341266788553779\n",
      "step =  16000 ,  loss_val =  1.0004373584117052\n",
      "step =  17000 ,  loss_val =  1.1534898436678396\n",
      "step =  18000 ,  loss_val =  0.9932817764183379\n",
      "step =  19000 ,  loss_val =  0.9028570416739106\n",
      "step =  20000 ,  loss_val =  1.0355653672619118\n",
      "step =  21000 ,  loss_val =  1.088784464234189\n",
      "step =  22000 ,  loss_val =  0.9764846089300363\n",
      "step =  23000 ,  loss_val =  1.0862419454560293\n",
      "step =  24000 ,  loss_val =  1.0111071751613703\n",
      "step =  25000 ,  loss_val =  1.1119602318231605\n",
      "step =  26000 ,  loss_val =  1.0061857186001801\n",
      "step =  27000 ,  loss_val =  1.005955912778684\n",
      "step =  28000 ,  loss_val =  0.9872595275112058\n",
      "step =  29000 ,  loss_val =  0.9971841592325021\n",
      "step =  30000 ,  loss_val =  1.1690901687045534\n",
      "step =  31000 ,  loss_val =  7.428766371573084\n",
      "step =  32000 ,  loss_val =  1.0316422756114279\n",
      "step =  33000 ,  loss_val =  1.0553353721596843\n",
      "step =  34000 ,  loss_val =  1.1006596939529127\n",
      "step =  35000 ,  loss_val =  0.9762068544113129\n",
      "step =  36000 ,  loss_val =  0.9706442971391913\n",
      "step =  37000 ,  loss_val =  1.093019655270576\n",
      "step =  38000 ,  loss_val =  1.0278751032883857\n",
      "step =  39000 ,  loss_val =  1.0957380608116325\n",
      "step =  40000 ,  loss_val =  1.1030172907496354\n",
      "step =  41000 ,  loss_val =  1.0710399938645279\n",
      "step =  42000 ,  loss_val =  1.0014284949953107\n",
      "step =  43000 ,  loss_val =  1.3069954268565787\n",
      "step =  44000 ,  loss_val =  1.153739185772715\n",
      "step =  45000 ,  loss_val =  1.1213438105408087\n",
      "step =  46000 ,  loss_val =  1.0247551618510986\n",
      "step =  47000 ,  loss_val =  1.1371437818962975\n",
      "step =  48000 ,  loss_val =  1.1259757901981189\n",
      "step =  49000 ,  loss_val =  1.1665756445259967\n",
      "step =  50000 ,  loss_val =  1.1438141361652452\n",
      "step =  51000 ,  loss_val =  1.084175692414559\n",
      "step =  52000 ,  loss_val =  1.0886018786445957\n",
      "step =  53000 ,  loss_val =  1.1292510954885324\n",
      "step =  54000 ,  loss_val =  1.1217752128287866\n",
      "step =  55000 ,  loss_val =  0.948802293337755\n",
      "step =  56000 ,  loss_val =  0.9845405135961816\n",
      "step =  57000 ,  loss_val =  1.2513176663227483\n",
      "step =  58000 ,  loss_val =  1.0951351777032596\n",
      "step =  59000 ,  loss_val =  1.1594169101920697\n",
      "step =  0 ,  loss_val =  1.199280074410377\n",
      "step =  1000 ,  loss_val =  1.2455312304976425\n",
      "step =  2000 ,  loss_val =  1.438290538195206\n",
      "step =  3000 ,  loss_val =  1.0992470847350755\n",
      "step =  4000 ,  loss_val =  1.193074269754251\n",
      "step =  5000 ,  loss_val =  1.1585763601120163\n",
      "step =  6000 ,  loss_val =  1.2309538551756969\n",
      "step =  7000 ,  loss_val =  1.0786654877608646\n",
      "step =  8000 ,  loss_val =  1.2821465123070808\n",
      "step =  9000 ,  loss_val =  1.1755843754552446\n",
      "step =  10000 ,  loss_val =  1.2733650954743636\n",
      "step =  11000 ,  loss_val =  1.1645199569411637\n",
      "step =  12000 ,  loss_val =  1.1315441101226624\n",
      "step =  13000 ,  loss_val =  1.1499350716673016\n",
      "step =  14000 ,  loss_val =  1.0889017749345258\n",
      "step =  15000 ,  loss_val =  1.1030866133229795\n",
      "step =  16000 ,  loss_val =  1.1048486610043016\n",
      "step =  17000 ,  loss_val =  1.1814727203645883\n",
      "step =  18000 ,  loss_val =  1.1554330484882922\n",
      "step =  19000 ,  loss_val =  1.0657118040125997\n",
      "step =  20000 ,  loss_val =  1.1772925071509182\n",
      "step =  21000 ,  loss_val =  1.2507225791579604\n",
      "step =  22000 ,  loss_val =  1.141084630000648\n",
      "step =  23000 ,  loss_val =  1.335038144826931\n",
      "step =  24000 ,  loss_val =  1.1910991080769597\n",
      "step =  25000 ,  loss_val =  1.2505026056953186\n",
      "step =  26000 ,  loss_val =  1.1631209062912136\n",
      "step =  27000 ,  loss_val =  1.0839020154413415\n",
      "step =  28000 ,  loss_val =  1.014177587781038\n",
      "step =  29000 ,  loss_val =  1.1999993032081118\n",
      "step =  30000 ,  loss_val =  1.2412170432934675\n",
      "step =  31000 ,  loss_val =  2.253435515021293\n",
      "step =  32000 ,  loss_val =  1.1888878621006511\n",
      "step =  33000 ,  loss_val =  1.128227756501359\n",
      "step =  34000 ,  loss_val =  1.2346577848229767\n",
      "step =  35000 ,  loss_val =  1.0672412722972593\n",
      "step =  36000 ,  loss_val =  1.015997560252705\n",
      "step =  37000 ,  loss_val =  1.2140772473495423\n",
      "step =  38000 ,  loss_val =  1.0907197912989575\n",
      "step =  39000 ,  loss_val =  1.086885250069939\n",
      "step =  40000 ,  loss_val =  1.2031756609428412\n",
      "step =  41000 ,  loss_val =  1.1180466525769592\n",
      "step =  42000 ,  loss_val =  1.058355586631222\n",
      "step =  43000 ,  loss_val =  1.0760737558939477\n",
      "step =  44000 ,  loss_val =  1.2214786640425845\n",
      "step =  45000 ,  loss_val =  1.1723145873939966\n",
      "step =  46000 ,  loss_val =  1.1253039767383572\n",
      "step =  47000 ,  loss_val =  1.2300434721141893\n",
      "step =  48000 ,  loss_val =  1.1577143846284779\n",
      "step =  49000 ,  loss_val =  1.2264168172287269\n",
      "step =  50000 ,  loss_val =  1.1978210716189697\n",
      "step =  51000 ,  loss_val =  1.1894164436863643\n",
      "step =  52000 ,  loss_val =  1.0984484363365856\n",
      "step =  53000 ,  loss_val =  1.2364902045847022\n",
      "step =  54000 ,  loss_val =  1.21466491342905\n",
      "step =  55000 ,  loss_val =  1.038397594054011\n",
      "step =  56000 ,  loss_val =  1.0482976365402863\n",
      "step =  57000 ,  loss_val =  1.32952260777076\n",
      "step =  58000 ,  loss_val =  1.0986098241612345\n",
      "step =  59000 ,  loss_val =  1.2237637915186104\n",
      "step =  0 ,  loss_val =  1.2842391934565558\n",
      "step =  1000 ,  loss_val =  1.2354883371885774\n",
      "step =  2000 ,  loss_val =  1.0359018204797512\n",
      "step =  3000 ,  loss_val =  1.0851131710304105\n",
      "step =  4000 ,  loss_val =  1.2402951073891069\n",
      "step =  5000 ,  loss_val =  1.2925603212223888\n",
      "step =  6000 ,  loss_val =  1.2344391188526962\n",
      "step =  7000 ,  loss_val =  1.1587878426359153\n",
      "step =  8000 ,  loss_val =  1.3047588004268882\n",
      "step =  9000 ,  loss_val =  1.274277892497738\n",
      "step =  10000 ,  loss_val =  1.2898512200679695\n",
      "step =  11000 ,  loss_val =  1.1909261136572227\n",
      "step =  12000 ,  loss_val =  1.2160559242037645\n",
      "step =  13000 ,  loss_val =  1.193337843988718\n",
      "step =  14000 ,  loss_val =  1.0514012973305684\n",
      "step =  15000 ,  loss_val =  1.1370115928611666\n",
      "step =  16000 ,  loss_val =  1.1978915146930667\n",
      "step =  17000 ,  loss_val =  1.3142911241127397\n",
      "step =  18000 ,  loss_val =  1.1748153357930642\n",
      "step =  19000 ,  loss_val =  1.1288200115730054\n",
      "step =  20000 ,  loss_val =  1.3065174800267871\n",
      "step =  21000 ,  loss_val =  1.2908239269555701\n",
      "step =  22000 ,  loss_val =  1.1964449925958822\n",
      "step =  23000 ,  loss_val =  1.4094813016980348\n",
      "step =  24000 ,  loss_val =  1.1399210004391507\n",
      "step =  25000 ,  loss_val =  1.2359428065027176\n",
      "step =  26000 ,  loss_val =  1.2455407574024415\n",
      "step =  27000 ,  loss_val =  1.1158648529601318\n",
      "step =  28000 ,  loss_val =  1.1337396061216727\n",
      "step =  29000 ,  loss_val =  1.2837109831981863\n",
      "step =  30000 ,  loss_val =  1.2054648957008827\n",
      "step =  31000 ,  loss_val =  1.096137772633198\n",
      "step =  32000 ,  loss_val =  1.2545777296933027\n",
      "step =  33000 ,  loss_val =  1.126663530110671\n",
      "step =  34000 ,  loss_val =  1.1881063641277472\n",
      "step =  35000 ,  loss_val =  1.1692714921002387\n",
      "step =  36000 ,  loss_val =  0.9967568040725672\n",
      "step =  37000 ,  loss_val =  1.2552473923239689\n",
      "step =  38000 ,  loss_val =  1.1773395242702298\n",
      "step =  39000 ,  loss_val =  1.1035677375921042\n",
      "step =  40000 ,  loss_val =  1.318909186826302\n",
      "step =  41000 ,  loss_val =  1.1238821150492446\n",
      "step =  42000 ,  loss_val =  1.1253823240995187\n",
      "step =  43000 ,  loss_val =  1.1205735136795334\n",
      "step =  44000 ,  loss_val =  1.3316634352994265\n",
      "step =  45000 ,  loss_val =  1.2757851521890016\n",
      "step =  46000 ,  loss_val =  1.1721343866997154\n",
      "step =  47000 ,  loss_val =  1.3364754783285355\n",
      "step =  48000 ,  loss_val =  1.2122355286790425\n",
      "step =  49000 ,  loss_val =  1.1778502649028764\n",
      "step =  50000 ,  loss_val =  1.2816204580406343\n",
      "step =  51000 ,  loss_val =  1.2364072827920183\n",
      "step =  52000 ,  loss_val =  1.193552090170452\n",
      "step =  53000 ,  loss_val =  1.1882104805535498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  54000 ,  loss_val =  1.188794764463203\n",
      "step =  55000 ,  loss_val =  1.0555769872671776\n",
      "step =  56000 ,  loss_val =  1.0448509152029342\n",
      "step =  57000 ,  loss_val =  1.399091881300637\n",
      "step =  58000 ,  loss_val =  1.1189122047357758\n",
      "step =  59000 ,  loss_val =  1.2809327450992904\n",
      "step =  0 ,  loss_val =  1.2509578765719\n",
      "step =  1000 ,  loss_val =  1.316643890193839\n",
      "step =  2000 ,  loss_val =  1.0704114465866699\n",
      "step =  3000 ,  loss_val =  1.1775631411154555\n",
      "step =  4000 ,  loss_val =  1.3025986180232292\n",
      "step =  5000 ,  loss_val =  1.2078358163832223\n",
      "step =  6000 ,  loss_val =  1.210357122870873\n",
      "step =  7000 ,  loss_val =  1.209392522910594\n",
      "step =  8000 ,  loss_val =  1.3553170886546315\n",
      "step =  9000 ,  loss_val =  1.3008180261611855\n",
      "step =  10000 ,  loss_val =  1.2951827377248168\n",
      "step =  11000 ,  loss_val =  1.2256088150999602\n",
      "step =  12000 ,  loss_val =  1.255761445265711\n",
      "step =  13000 ,  loss_val =  1.2214347533992354\n",
      "step =  14000 ,  loss_val =  1.1170704246406598\n",
      "step =  15000 ,  loss_val =  1.2162935549748852\n",
      "step =  16000 ,  loss_val =  1.2367172835792333\n",
      "step =  17000 ,  loss_val =  1.302492662166743\n",
      "step =  18000 ,  loss_val =  1.1521359277304974\n",
      "step =  19000 ,  loss_val =  1.1513595351042945\n",
      "step =  20000 ,  loss_val =  1.3676927940091466\n",
      "step =  21000 ,  loss_val =  1.270801293802682\n",
      "step =  22000 ,  loss_val =  1.232632843113801\n",
      "step =  23000 ,  loss_val =  1.5052751826767872\n",
      "step =  24000 ,  loss_val =  1.237008808697838\n",
      "step =  25000 ,  loss_val =  1.370351401461549\n",
      "step =  26000 ,  loss_val =  1.29606925948366\n",
      "step =  27000 ,  loss_val =  1.134117116472659\n",
      "step =  28000 ,  loss_val =  1.2163966707148288\n",
      "step =  29000 ,  loss_val =  1.3939886729811473\n",
      "step =  30000 ,  loss_val =  1.2591492444454329\n",
      "step =  31000 ,  loss_val =  1.1846161087573681\n",
      "step =  32000 ,  loss_val =  1.2979908878349034\n",
      "step =  33000 ,  loss_val =  1.2096619586154862\n",
      "step =  34000 ,  loss_val =  1.26054827497334\n",
      "step =  35000 ,  loss_val =  1.2057090083244337\n",
      "step =  36000 ,  loss_val =  1.0990117685026863\n",
      "step =  37000 ,  loss_val =  1.2679519919365516\n",
      "step =  38000 ,  loss_val =  1.2153461194237913\n",
      "step =  39000 ,  loss_val =  1.1286062096797085\n",
      "step =  40000 ,  loss_val =  1.3260422213657521\n",
      "step =  41000 ,  loss_val =  1.1642951084824464\n",
      "step =  42000 ,  loss_val =  1.1883557129080915\n",
      "step =  43000 ,  loss_val =  1.1523822171639155\n",
      "step =  44000 ,  loss_val =  1.3401779614125355\n",
      "step =  45000 ,  loss_val =  1.245093726833882\n",
      "step =  46000 ,  loss_val =  1.171135478145727\n",
      "step =  47000 ,  loss_val =  1.3012619481369454\n",
      "step =  48000 ,  loss_val =  1.3384875450371487\n",
      "step =  49000 ,  loss_val =  1.2792755628581316\n",
      "step =  50000 ,  loss_val =  1.3047683077939958\n",
      "step =  51000 ,  loss_val =  1.3152691081155254\n",
      "step =  52000 ,  loss_val =  1.2065701470787291\n",
      "step =  53000 ,  loss_val =  1.3296566022549896\n",
      "step =  54000 ,  loss_val =  1.2965435542005603\n",
      "step =  55000 ,  loss_val =  1.0670638637021304\n",
      "step =  56000 ,  loss_val =  1.1382694965674816\n",
      "step =  57000 ,  loss_val =  1.4140293458702409\n",
      "step =  58000 ,  loss_val =  1.1545797658359922\n",
      "step =  59000 ,  loss_val =  1.3200608393904116\n",
      "step =  0 ,  loss_val =  1.2811008451051957\n",
      "step =  1000 ,  loss_val =  1.2443852429446165\n",
      "step =  2000 ,  loss_val =  1.0686316374601432\n",
      "step =  3000 ,  loss_val =  1.1757020157929279\n",
      "step =  4000 ,  loss_val =  1.3341122066102133\n",
      "step =  5000 ,  loss_val =  1.3768319872885073\n",
      "step =  6000 ,  loss_val =  1.30015882730501\n",
      "step =  7000 ,  loss_val =  1.2557281430748497\n",
      "step =  8000 ,  loss_val =  1.3689737978658172\n",
      "step =  9000 ,  loss_val =  1.302802488859414\n",
      "step =  10000 ,  loss_val =  1.388691290271755\n",
      "step =  11000 ,  loss_val =  1.2103917960910862\n",
      "step =  12000 ,  loss_val =  1.269576171444279\n",
      "step =  13000 ,  loss_val =  1.2873647718479744\n",
      "step =  14000 ,  loss_val =  1.172114451095001\n",
      "step =  15000 ,  loss_val =  1.2903231734812375\n",
      "step =  16000 ,  loss_val =  1.267265762114949\n",
      "step =  17000 ,  loss_val =  1.3178421849975732\n",
      "step =  18000 ,  loss_val =  1.1137486256811244\n",
      "step =  19000 ,  loss_val =  1.2262291664976372\n",
      "step =  20000 ,  loss_val =  1.4360548646693632\n",
      "step =  21000 ,  loss_val =  1.3064373630498693\n",
      "step =  22000 ,  loss_val =  1.2348133153830987\n",
      "step =  23000 ,  loss_val =  1.5110421486616383\n",
      "step =  24000 ,  loss_val =  1.250138838048943\n",
      "step =  25000 ,  loss_val =  1.3881050044055485\n",
      "step =  26000 ,  loss_val =  1.3247495071720856\n",
      "step =  27000 ,  loss_val =  1.1715696144310117\n",
      "step =  28000 ,  loss_val =  1.136480914784551\n",
      "step =  29000 ,  loss_val =  1.3131343273553677\n",
      "step =  30000 ,  loss_val =  1.3454652845068527\n",
      "step =  31000 ,  loss_val =  1.2190261250013052\n",
      "step =  32000 ,  loss_val =  1.3272465958984843\n",
      "step =  33000 ,  loss_val =  1.1766284613056337\n",
      "step =  34000 ,  loss_val =  1.30073042485874\n",
      "step =  35000 ,  loss_val =  1.1926892745086473\n",
      "step =  36000 ,  loss_val =  1.1070735630215098\n",
      "step =  37000 ,  loss_val =  1.3661445403517798\n",
      "step =  38000 ,  loss_val =  1.2358892908472094\n",
      "step =  39000 ,  loss_val =  1.1807422258692852\n",
      "step =  40000 ,  loss_val =  1.3409874322248883\n",
      "step =  41000 ,  loss_val =  1.183151949988358\n",
      "step =  42000 ,  loss_val =  1.1855739426922483\n",
      "step =  43000 ,  loss_val =  1.15869462797054\n",
      "step =  44000 ,  loss_val =  1.4317350184753401\n",
      "step =  45000 ,  loss_val =  1.327394529501226\n",
      "step =  46000 ,  loss_val =  1.2022748044517857\n",
      "step =  47000 ,  loss_val =  1.3819606738164414\n",
      "step =  48000 ,  loss_val =  1.3532000229758407\n",
      "step =  49000 ,  loss_val =  1.2830099091754512\n",
      "step =  50000 ,  loss_val =  1.369021963828739\n",
      "step =  51000 ,  loss_val =  1.2889355367918123\n",
      "step =  52000 ,  loss_val =  1.2628223185020107\n",
      "step =  53000 ,  loss_val =  1.299290835607115\n",
      "step =  54000 ,  loss_val =  1.2986181868258242\n",
      "step =  55000 ,  loss_val =  1.1244879511731045\n",
      "step =  56000 ,  loss_val =  1.1698537674820322\n",
      "step =  57000 ,  loss_val =  1.460491779037079\n",
      "step =  58000 ,  loss_val =  1.212271198341562\n",
      "step =  59000 ,  loss_val =  1.3338100943434077\n",
      "\n",
      "elapsed time =  0:00:35.916051\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "learning_rate = 0.3\n",
    "epochs = 5\n",
    "\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(output_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "    \n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )\n",
    "    \n",
    "        if step % 1000 == 0:\n",
    "            print(\"step = \", step,  \",  loss_val = \", nn.loss_val())\n",
    "        \n",
    "end_time = datetime.now() \n",
    "print(\"\\nelapsed time = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Accuracy =  96.53  %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[149,\n",
       " 151,\n",
       " 241,\n",
       " 247,\n",
       " 257,\n",
       " 259,\n",
       " 320,\n",
       " 321,\n",
       " 340,\n",
       " 381,\n",
       " 445,\n",
       " 447,\n",
       " 448,\n",
       " 479,\n",
       " 495,\n",
       " 565,\n",
       " 582,\n",
       " 591,\n",
       " 659,\n",
       " 691,\n",
       " 707,\n",
       " 717,\n",
       " 720,\n",
       " 740,\n",
       " 760,\n",
       " 810,\n",
       " 844,\n",
       " 924,\n",
       " 938,\n",
       " 939,\n",
       " 947,\n",
       " 950,\n",
       " 956,\n",
       " 965,\n",
       " 1003,\n",
       " 1014,\n",
       " 1044,\n",
       " 1107,\n",
       " 1112,\n",
       " 1181,\n",
       " 1182,\n",
       " 1194,\n",
       " 1204,\n",
       " 1224,\n",
       " 1226,\n",
       " 1232,\n",
       " 1242,\n",
       " 1247,\n",
       " 1260,\n",
       " 1283,\n",
       " 1289,\n",
       " 1299,\n",
       " 1319,\n",
       " 1326,\n",
       " 1328,\n",
       " 1337,\n",
       " 1393,\n",
       " 1395,\n",
       " 1425,\n",
       " 1494,\n",
       " 1500,\n",
       " 1522,\n",
       " 1530,\n",
       " 1549,\n",
       " 1553,\n",
       " 1581,\n",
       " 1609,\n",
       " 1626,\n",
       " 1678,\n",
       " 1681,\n",
       " 1709,\n",
       " 1717,\n",
       " 1722,\n",
       " 1754,\n",
       " 1790,\n",
       " 1878,\n",
       " 1901,\n",
       " 1930,\n",
       " 1938,\n",
       " 1940,\n",
       " 1952,\n",
       " 1969,\n",
       " 1982,\n",
       " 1984,\n",
       " 2016,\n",
       " 2035,\n",
       " 2043,\n",
       " 2044,\n",
       " 2053,\n",
       " 2070,\n",
       " 2093,\n",
       " 2098,\n",
       " 2109,\n",
       " 2118,\n",
       " 2125,\n",
       " 2129,\n",
       " 2130,\n",
       " 2135,\n",
       " 2182,\n",
       " 2186,\n",
       " 2189,\n",
       " 2215,\n",
       " 2224,\n",
       " 2266,\n",
       " 2272,\n",
       " 2293,\n",
       " 2299,\n",
       " 2369,\n",
       " 2371,\n",
       " 2380,\n",
       " 2387,\n",
       " 2414,\n",
       " 2422,\n",
       " 2433,\n",
       " 2447,\n",
       " 2454,\n",
       " 2462,\n",
       " 2488,\n",
       " 2514,\n",
       " 2574,\n",
       " 2598,\n",
       " 2607,\n",
       " 2648,\n",
       " 2654,\n",
       " 2721,\n",
       " 2780,\n",
       " 2802,\n",
       " 2810,\n",
       " 2877,\n",
       " 2896,\n",
       " 2907,\n",
       " 2927,\n",
       " 2939,\n",
       " 2995,\n",
       " 3030,\n",
       " 3060,\n",
       " 3073,\n",
       " 3117,\n",
       " 3130,\n",
       " 3136,\n",
       " 3157,\n",
       " 3167,\n",
       " 3189,\n",
       " 3206,\n",
       " 3225,\n",
       " 3240,\n",
       " 3269,\n",
       " 3289,\n",
       " 3329,\n",
       " 3330,\n",
       " 3336,\n",
       " 3376,\n",
       " 3405,\n",
       " 3422,\n",
       " 3503,\n",
       " 3520,\n",
       " 3533,\n",
       " 3550,\n",
       " 3558,\n",
       " 3559,\n",
       " 3567,\n",
       " 3597,\n",
       " 3604,\n",
       " 3662,\n",
       " 3681,\n",
       " 3718,\n",
       " 3757,\n",
       " 3767,\n",
       " 3776,\n",
       " 3780,\n",
       " 3808,\n",
       " 3811,\n",
       " 3817,\n",
       " 3838,\n",
       " 3848,\n",
       " 3853,\n",
       " 3869,\n",
       " 3876,\n",
       " 3893,\n",
       " 3902,\n",
       " 3906,\n",
       " 3926,\n",
       " 3941,\n",
       " 3943,\n",
       " 3976,\n",
       " 4017,\n",
       " 4027,\n",
       " 4063,\n",
       " 4075,\n",
       " 4078,\n",
       " 4152,\n",
       " 4163,\n",
       " 4176,\n",
       " 4193,\n",
       " 4199,\n",
       " 4205,\n",
       " 4211,\n",
       " 4224,\n",
       " 4248,\n",
       " 4255,\n",
       " 4265,\n",
       " 4289,\n",
       " 4294,\n",
       " 4306,\n",
       " 4433,\n",
       " 4435,\n",
       " 4497,\n",
       " 4498,\n",
       " 4500,\n",
       " 4536,\n",
       " 4540,\n",
       " 4571,\n",
       " 4575,\n",
       " 4578,\n",
       " 4601,\n",
       " 4615,\n",
       " 4619,\n",
       " 4639,\n",
       " 4699,\n",
       " 4731,\n",
       " 4740,\n",
       " 4751,\n",
       " 4807,\n",
       " 4814,\n",
       " 4823,\n",
       " 4837,\n",
       " 4874,\n",
       " 4876,\n",
       " 4879,\n",
       " 4880,\n",
       " 4886,\n",
       " 4943,\n",
       " 4950,\n",
       " 4952,\n",
       " 4956,\n",
       " 4966,\n",
       " 4981,\n",
       " 4990,\n",
       " 5140,\n",
       " 5159,\n",
       " 5246,\n",
       " 5331,\n",
       " 5457,\n",
       " 5600,\n",
       " 5634,\n",
       " 5642,\n",
       " 5676,\n",
       " 5734,\n",
       " 5749,\n",
       " 5835,\n",
       " 5842,\n",
       " 5887,\n",
       " 5888,\n",
       " 5926,\n",
       " 5936,\n",
       " 5937,\n",
       " 5955,\n",
       " 5972,\n",
       " 5973,\n",
       " 5982,\n",
       " 6011,\n",
       " 6024,\n",
       " 6035,\n",
       " 6053,\n",
       " 6056,\n",
       " 6059,\n",
       " 6071,\n",
       " 6081,\n",
       " 6091,\n",
       " 6157,\n",
       " 6166,\n",
       " 6172,\n",
       " 6173,\n",
       " 6391,\n",
       " 6505,\n",
       " 6555,\n",
       " 6571,\n",
       " 6577,\n",
       " 6597,\n",
       " 6598,\n",
       " 6608,\n",
       " 6625,\n",
       " 6632,\n",
       " 6645,\n",
       " 6651,\n",
       " 6706,\n",
       " 6721,\n",
       " 6744,\n",
       " 6906,\n",
       " 7216,\n",
       " 7338,\n",
       " 7432,\n",
       " 7434,\n",
       " 7451,\n",
       " 7539,\n",
       " 7797,\n",
       " 7851,\n",
       " 7886,\n",
       " 7902,\n",
       " 7917,\n",
       " 7921,\n",
       " 7945,\n",
       " 8047,\n",
       " 8091,\n",
       " 8094,\n",
       " 8272,\n",
       " 8406,\n",
       " 8408,\n",
       " 8520,\n",
       " 8522,\n",
       " 9009,\n",
       " 9015,\n",
       " 9019,\n",
       " 9024,\n",
       " 9046,\n",
       " 9280,\n",
       " 9422,\n",
       " 9513,\n",
       " 9587,\n",
       " 9634,\n",
       " 9664,\n",
       " 9669,\n",
       " 9679,\n",
       " 9700,\n",
       " 9716,\n",
       " 9729,\n",
       " 9744,\n",
       " 9745,\n",
       " 9749,\n",
       " 9752,\n",
       " 9768,\n",
       " 9770,\n",
       " 9777,\n",
       " 9779,\n",
       " 9792,\n",
       " 9808,\n",
       " 9839,\n",
       " 9858,\n",
       " 9867,\n",
       " 9883,\n",
       " 9888,\n",
       " 9904,\n",
       " 9905,\n",
       " 9941,\n",
       " 9944,\n",
       " 9980,\n",
       " 9982]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(test_data)   \n",
    "# epochs == 1 인 경우 accuracy 94.03%\n",
    "# epochs == 5 인 경우 accuracy 96.44%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value is \n",
      "\n",
      "0 \n",
      "\n",
      "the answer is \n",
      "\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = test_data[9888][1:].reshape(28,28)\n",
    "\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "#predicted value\n",
    "print(\"predicted value is\", '\\n')\n",
    "print(nn.predict(test_data[9888][1:]), '\\n')\n",
    "\n",
    "print (\"the answer is\" ,'\\n')\n",
    "print (test_data[9888][0])\n",
    "\n",
    "#사람이 헷갈릴만한 데이터를 틀리기도 하지만 당연스러운것도 틀리기도한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
