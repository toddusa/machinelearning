{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#소스코드 링크: https://github.com/sr6033/Deep-learning-model-for-stock-price-prediction\n",
    "\n",
    "#import 하는 라이브러리들을 간단한 축약 용어로 import 해줌\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data_stocks.csv' does not exist: b'data_stocks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d3406012ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_stocks.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# drop the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# pandas의 drop은 하나의 행 또는 열 삭제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data_stocks.csv' does not exist: b'data_stocks.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import data\n",
    "data = pd.read_csv('data_stocks.csv')\n",
    "\n",
    "# drop the data\n",
    "# pandas의 drop은 하나의 행 또는 열 삭제\n",
    "# Data.drop( list, axis=0/1)\n",
    "# drop 하고자 하는 행의 색인, 열의 이름을 값이나 list로 입력하며 axis=1이면 열을 drop 하는 것으로 함\n",
    "# defualt는 행(0)임\n",
    "data = data.drop(['DATE'], 1)\n",
    "# data 내부에 ['DATE']라는 첫번째 열 삭제\n",
    "\n",
    "# dimensions of dataset\n",
    "# 사용하고자 하는 데이터의 크기를 확인하는데 사용\n",
    "\n",
    "n = data.shape[0] #행의 갯수\n",
    "p = data.shape[1] #열의 갯수\n",
    "\n",
    "print(\"Dimensions of [0] is\", n)\n",
    "print(\"Dimensions of [1] is\", p)\n",
    "\n",
    "# make data as numpy array\n",
    "data = data.values\n",
    "# 기존 엑셀 파일의 첫번째 행을 key로 인식해 dictionary type으로 생각해, \n",
    "# values를 불러왔을 때 첫번째 행을 제외한 값들을 numpy array형식으로 받음\n",
    "\n",
    "print(\"data\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing data\n",
    "\n",
    "train_start = 0\n",
    "train_end = int(np.floor(0.8*n)) # 파이썬 올림(ceil), 내림(floor), 반올림(round)\n",
    "                                 # int: integer\n",
    "test_start = train_end\n",
    "test_end = n\n",
    "\n",
    "#총 n (csv 파일의 행의수) 개의 데이터 중에 대략 80퍼센트를 train data 로 사용\n",
    "\n",
    "# numpy의 arrange 함수\n",
    "# numpy.arange([start, ] stop, [step, ] dtype=None)\n",
    "# :start와 stop에 step의 크기만큼 일정하게 떨어져 있는 숫자들을 array 형태로 반환\n",
    "# :stop 값은 반드시 명시되어야 하며, start와 step은 지정하지 않으면 각각 0과 1을 default로 가짐.\n",
    "\n",
    "data_train = data[np.arange(train_start, train_end), :] # 슬라이싱을 이용해 train end 지점까지의 데이터를 행렬형식으로 모두 받음.\n",
    "data_test = data[np.arange(test_start, test_end), :]\n",
    "\n",
    "# 구체적으로 현재 코드에서 np.arrange가 어떻게 쓰였는지 확인하기 위해 추가\n",
    "\n",
    "print(\"np.arange(train_start, train_end) is\", '\\n')\n",
    "print(np.arange(train_start, train_end), '\\n')\n",
    "\n",
    "print(\"data_train is\", '\\n')\n",
    "print(data_train, '\\n')\n",
    "print(\"data_test is\", '\\n')\n",
    "print(data_test)\n",
    " \n",
    "# 여기까지는 임의의 값 n에 대한 정의가 없다. 나중에 지정해주나보다/// n 은 전 n = data.shape[0] #행의 갯수로 정의함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn의 전처리 기능\n",
    "# 스케일링(scale): 모든 자료에 선형 변환을 적용해 전체 자료 분포를 평균 0, 분산 1이 되도록 만들어줌. \n",
    "# 최적화 과정에서 안정성 및 수렴 속도를 향상시키는 장점\n",
    "# StandardScaler(x): 평균이 0과 표준편차가 1이 되도록 변환\n",
    "# RobustScaler(x): 중앙값(median)이 0, IQR(interquartile range)이 1이 되도록 변환\n",
    "# MinMaxScaler(x): 최댓값이 각각 1, 최솟값이 0이 되도록 변환\n",
    "# MaxAbsScaler(x): 0을 기준으로 절댓값이 가장 큰수가 1 또는 -1이 되도록 변환\n",
    "# 참고 링크: https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() # MinMaxScaler를 통해 데이터 1~0 사잇값으로 변환\n",
    "\n",
    "\n",
    "\n",
    "#scaler.fit(data_train) # fit 함수를 통해 분포 모수를 객체 내에 저장, 데이터가 0과 1사이 값으로 바뀜\n",
    "scaler.fit(data_test)\n",
    "\n",
    "print(\"scaler.fit(data_train) is\", '\\n', scaler.fit(data_train), '\\n') \n",
    "\n",
    "print(\"data_train is\", '\\n',data_train, '\\n')\n",
    "\n",
    "\n",
    "data_train = scaler.transform(data_train) # transform 함수를 통해 data를 변환\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "print(\"scaler.transform(data_train) is\", '\\n')\n",
    "print(scaler.transform(data_train), '\\n')\n",
    "print( \"new data_train is\", '\\n',data_train, '\\n')\n",
    "print(\"scaler.transform(data_test) is\", '\\n')\n",
    "print(scaler.transform(data_test), '\\n')\n",
    "print(\"new data_train is\", '\\n',data_test, '\\n')\n",
    "\n",
    "#궁금한점? data_train은 fit 과정을 거치면서 왜 data_test는 fit 과정을 거치지 않았을까?\n",
    "# MinMaxScaler.fit 과정에서 data_train 의 최대 최솟값을 0, 1 로 정의하게 되는데, 이값을 레퍼런스로 사용해야하기때문임\n",
    "# data_test 도 fit 을 해버리면 0과 1사이의값을 가지게 될거고 데이터가 왜곡됨.\n",
    "# 이라고 생각했지만 scaler.fit(data_train) 을 scaler.fit(data_test) 로 바꾸고 실행해도 결과가 똑같음;; 멘붕\n",
    "# 했지만 엄청난 노가다후에 print 안에있는 print(scaler.fit(data_train) 자체가 프린트만하는게 아니라 걍 scaler.fit 을실행시켜버린다는걸 알게됨\n",
    "#print(scaler.fit(data_test) 로 바꾸면 결과가 data_test 에 피팅되어있는걸 확인할 수 있음\n",
    "\n",
    "#궁금한점? 왜 print (scaler.transform(data_test)) 와 print(data_test) 가 다르지.. cell run 한번 더하면 같아지는데 그건왜지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build X & Y\n",
    "X_train = data_train[:, 1:] # 1: (슬라이싱) 1을 기준으로 오른쪽으로 슬라이싱 한다는 뜻\n",
    "Y_train = data_train[:, 0]\n",
    "X_test = data_test[:, 1:]\n",
    "Y_test = data_test[:, 0]\n",
    "\n",
    "#Y 가 주식 가격 X 는 가격에 영향을 미치는 다양한 변수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Tensorflow의 Tensor란? 데이터의 배열 (Array)\n",
    "# Tensor에 대해 더 궁금하다면 : https://m.blog.naver.com/PostView.nhn?blogId=complusblog&logNo=221237818389&proxyReferer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "# tensorflow의 데이터 근원 \n",
    "# 플레이스 홀더 이외 나머지 참고 링크: https://datascienceschool.net/view-notebook/5cbab09d777841f591a67928d7043f51/\n",
    "# 1. 상수형(Constant): 미리 주어진 값으로 고정된 텐서\n",
    "# 2. 변수형(Variable): 세션 내에서 값이 바뀔수 있는 텐서\n",
    "# 3. 플레이스홀더(Placeholder): 고정된 값을 가지지만 값이 미리 주어지지 않고 나중에 넣을 수 있음.\n",
    "#   예시코드\n",
    "#   x=tf.placeholder(dtype=tf.int32, shape=(None, 10)) => 행의 갯수는 미정, 열의 갯수는 10개\n",
    "\n",
    "# Model architecture parameters\n",
    "\n",
    "n_stocks = 500 # 입력값의 노드는 입력 데이터의 갯수와 일치하게 설정 (정답을 제외한 1열을 빼면 500개)\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "n_target = 1\n",
    "# 히든 노드의 구성 (원하는 대로 설정하면됨)\n",
    "# 500개 입력 -> 1024개의 첫번째 뉴런 -> 512개의 두번째 뉴런 -> 256개의 세번째 뉴런 -> 128개의 4번째 뉴런 -> 1개의 output\n",
    "\n",
    "# Placeholder: 차후 임의의 값을 입력으로 받기 위한 정의\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])\n",
    "# X는 float32라는 형식의 데이터 타입을 가지며, 구조는 행의 갯수는 미정, 열의 갯수는 n_stocks 만큼 가짐.\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "# Initializers\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "print(\"weight_initializer is\")\n",
    "print(weight_initializer)\n",
    "\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "print(\"bias_initializer is\")\n",
    "print(bias_initializer)\n",
    "\n",
    "# 뭔진 모르겠지만 텐서플로우를 사용할때 파라미터 초기화 방법들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virEnvtesorflow",
   "language": "python",
   "name": "virenvtesorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
