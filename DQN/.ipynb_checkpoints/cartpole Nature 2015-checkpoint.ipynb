{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Ai Gym cartpole 을 nature 2015  방법으로 DQN 학습!\n",
    "\n",
    "\"https://www.nature.com/articles/nature14236\"\n",
    "\n",
    "Nature 2015 keypoint:\n",
    "\n",
    "기존에 \n",
    "\n",
    "1. Correlations between samples\n",
    "2. Non stationary target \n",
    "\n",
    "위와같은 이유로 Q 수렴성이 떨어지는 문제를\n",
    "\n",
    "1. Go deep *2013\n",
    "2. Capture and replay *2013\n",
    "3. Separate networks *2015\n",
    "\n",
    "의 해결방법으로 해결함\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\toddu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "#이거 확실히 v2 랑 v1 공부해야할듯\n",
    "# https://www.youtube.com/watch?v=FvAsdZwavpA\n",
    "# tensorflow2.0 이 좀더 직관적이고 hihg level API 인 keras 위주로 가는것같음\n",
    "# https://www.tensorflow.org/guide/migrate 이민가이드고.. 단하나 contrib 는 예욐ㅋㅋ\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN AI 의 Cartpole 환경 도입\n",
    "\n",
    "1. input_size = state 가 어떤걸 의미하는지 알수없는 4개의 숫자로 표현이됨. env.step(action)의 첫행임\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "\n",
    " <p>\n",
    "action = env.action_space.sample()\n",
    "    \n",
    "observation, reward, done, _ = env.step(action)\n",
    " </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "2. output_size = 왼쪽 혹은 오른쪽 2개\n",
    "3. discount factor = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env =gym.make('CartPole-v0')\n",
    "#env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size= env.action_space.n\n",
    "\n",
    "env._max_episode_steps = 10001\n",
    "\n",
    "# constants difining our neural network\n",
    "\n",
    "dis =0.9\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 10\n",
    "sigma = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN class 정의\n",
    "\n",
    "ref: https://github.com/hunkim/ReinforcementZeroToAll/blob/master/dqn.py 에서 수정\n",
    "\n",
    "# DQN class can\n",
    "\n",
    "1) Build network\\\n",
    "2) Predict Q_value given state\\\n",
    "3) Train parameters\n",
    "\n",
    "Args :\n",
    "\n",
    " session (tf.Session): Tensorflow session\\\n",
    " input_size (int): Input dimension\\\n",
    " output_size (int): Number of discrete actions\\\n",
    " name (str, optional): TF Graph will be built under this name scope\n",
    " \n",
    "# DQN Network architecture :\n",
    "\n",
    "Args:\n",
    "\n",
    " h_size (int, optional): Hidden layer dimension\\\n",
    " l_rate (float, optional): Learning rate\n",
    " \n",
    "acknowledgement:\n",
    "\n",
    "hidden layer 수 = 1\\\n",
    "hidden layer dimension/node 수 =10\\\n",
    "learning rate = 0.001\\\n",
    "activation function = tanh\\\n",
    "loss function = MSE\\\n",
    "optimizer = adam\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\"):\n",
    "        \n",
    "        # tensorflow 2.0.0버전에선 tf.Session 이사용되지않음.  tf.compat.v1.Session 로 변경 했다가 위에 import 를 아예 바꿔버림\n",
    "        \n",
    "        self.session =session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        self._build_network()\n",
    "        \n",
    "#===============================================================================================\n",
    "        \n",
    "    def _build_network(self, h_size = 10, l_rate = 0.001):\n",
    "    \n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder( shape=[None, self.input_size], name=\"input_x\", dtype=tf.float32)\n",
    "            \n",
    "            #first layer \n",
    "            W1= tf.get_variable(\"W1\", shape=[self.input_size, h_size], \n",
    "                                initializer=tf.keras.initializers.glorot_uniform())\n",
    "            \n",
    "            #initializer=tf.contrib.layers.xavier_initializer()) 였지만\n",
    "            # 나도 contrib 문제를 겪게되어 아영이의 initializer로 바꾸어 주었다.\n",
    "            # tensorflow_core.compat.v1 조차도 contrib API가 없어져ㅆ다..\n",
    "            #로할라했지만 contrib 없이 xavier Glorot (사람이름) 의 다른방법 = Glorot 이 있어서 그걸로 하였다.\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform?version=stable\n",
    "            \n",
    "            layer1 = tf.nn.tanh(tf.matmul(self._X, W1))\n",
    "            \n",
    "            #Second layer             \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, self.output_size], \n",
    "                                 initializer=tf.keras.initializers.glorot_uniform())\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Q_prediction\n",
    "            self._Qpred = tf.matmul(layer1, W2)\n",
    "            \n",
    "            \n",
    "            #we need to define the parts of the network needed for learning a policy\n",
    "            self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "            \n",
    "            #loss function                   \n",
    "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
    "\n",
    "            #learning            \n",
    "            self._train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self._loss)\n",
    "           \n",
    "            \n",
    "#===============================================================================================\n",
    "            \n",
    "    # Q(s,a) 을 리턴 하는 predict 함수\n",
    "    # state (np.ndarray): State array, shape (n, imput_dim)\n",
    "    # return np.ndarray: Q value array, shape (n, output_dim)\n",
    "    \n",
    "    \n",
    "    def predict(self, state: np.ndarray):\n",
    "        \n",
    "        x = np.reshape(state, [1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "    \n",
    "#===============================================================================================\n",
    "    # X, y 를 받아 업데이트 하고 loss, result from train step 을 리턴\n",
    "    \n",
    "    \n",
    "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray):\n",
    "      \n",
    "        return self.session.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from replay buffer (Simple Replay Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_replay_train(DQN,train_batch):\n",
    "    x_stack = np.empty(0).reshape(0, DQN.input_size)\n",
    "    y_stack = np.empty(0).reshape(0, DQN.output_size)\n",
    "    \n",
    "    #get stored information from the buffer\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q=DQN.predict(state)\n",
    "        \n",
    "        #terminal?\n",
    "        if done:\n",
    "            Q[0,action] = reward\n",
    "        else:\n",
    "            #obtain Q' values by feeding the new state through our network\n",
    "            Q[0,action] =reward + dis*np.max(DQN.predict(next_state))\n",
    "            \n",
    "        x_stack = np.vstack([x_stack, state])    \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        \n",
    "    #train our network using target and predicted Q values on each episode\n",
    "    return DQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 네트워크를 받아 play 를 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN):\n",
    "    s=env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s, reward, done, _ =env.step(a)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            env.close()  #안하면 응답없음 걸려버림\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    \n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = DQN(sess, input_size, output_size)\n",
    "        \n",
    "        tf.global_variables_initializer().run()    # Weight 초기화\n",
    "        \n",
    "       \n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = 1. / ((episode/ 10)+1) #decaying epsilon\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            step_count = 0\n",
    "            \n",
    "                    \n",
    "            while not done:\n",
    "                \n",
    "                #epillon 의 확률로 random action 을하고 나머지 확률로 argmax\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample() \n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "                    \n",
    "                #get new state, reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                #끝나지않는게 목적이므로 패널티\n",
    "                if done:\n",
    "                    reward = -10\n",
    "\n",
    "                    \n",
    "                #경험을 replay_buffer 에 저장. 위의 deque 에서 replay memory 크기를 50000으로 한정\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                \n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "                \n",
    "                       \n",
    "                \n",
    "            if episode % 10 ==1:\n",
    "                minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                loss, _ =simple_replay_train(mainDQN, minibatch)\n",
    "                \n",
    "                    \n",
    "            if episode % 50 ==1:\n",
    "                print(\"[Episode {:>5}]  steps: {:>5}\".format(episode, step_count))\n",
    "                print(\"Loss: \", loss)\n",
    "            \n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward >400:\n",
    "                    print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "          \n",
    "        \n",
    "        bot_play(mainDQN)\n",
    "        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     1]  steps:    16\n",
      "Loss:  0.85718906\n",
      "[Episode    51]  steps:    10\n",
      "Loss:  10.453459\n",
      "[Episode   101]  steps:    10\n",
      "Loss:  11.972549\n",
      "[Episode   151]  steps:    10\n",
      "Loss:  0.5619446\n",
      "[Episode   201]  steps:     8\n",
      "Loss:  11.994375\n",
      "[Episode   251]  steps:     9\n",
      "Loss:  11.729116\n",
      "[Episode   301]  steps:    11\n",
      "Loss:  11.865472\n",
      "[Episode   351]  steps:    10\n",
      "Loss:  0.65408665\n",
      "[Episode   401]  steps:    10\n",
      "Loss:  11.610555\n",
      "[Episode   451]  steps:     8\n",
      "Loss:  0.60439414\n",
      "[Episode   501]  steps:    10\n",
      "Loss:  0.54832697\n",
      "[Episode   551]  steps:     8\n",
      "Loss:  11.419909\n",
      "[Episode   601]  steps:    10\n",
      "Loss:  6.0255394\n",
      "[Episode   651]  steps:     8\n",
      "Loss:  16.972168\n",
      "[Episode   701]  steps:     9\n",
      "Loss:  5.9360957\n",
      "[Episode   751]  steps:    10\n",
      "Loss:  5.9396234\n",
      "[Episode   801]  steps:    10\n",
      "Loss:  5.872441\n",
      "[Episode   851]  steps:    10\n",
      "Loss:  11.233711\n",
      "[Episode   901]  steps:    10\n",
      "Loss:  0.5293664\n",
      "[Episode   951]  steps:     9\n",
      "Loss:  16.462896\n",
      "[Episode  1001]  steps:     9\n",
      "Loss:  11.043177\n",
      "[Episode  1051]  steps:    10\n",
      "Loss:  5.727523\n",
      "[Episode  1101]  steps:     8\n",
      "Loss:  10.847958\n",
      "[Episode  1151]  steps:    10\n",
      "Loss:  10.805326\n",
      "[Episode  1201]  steps:     8\n",
      "Loss:  0.5112108\n",
      "[Episode  1251]  steps:     9\n",
      "Loss:  0.51035726\n",
      "[Episode  1301]  steps:    10\n",
      "Loss:  15.765528\n",
      "[Episode  1351]  steps:    10\n",
      "Loss:  5.5292926\n",
      "[Episode  1401]  steps:     9\n",
      "Loss:  10.545912\n",
      "[Episode  1451]  steps:    10\n",
      "Loss:  10.485123\n",
      "[Episode  1501]  steps:     9\n",
      "Loss:  15.459524\n",
      "[Episode  1551]  steps:     9\n",
      "Loss:  5.5140185\n",
      "[Episode  1601]  steps:    10\n",
      "Loss:  5.488619\n",
      "[Episode  1651]  steps:    10\n",
      "Loss:  0.50518095\n",
      "[Episode  1701]  steps:    12\n",
      "Loss:  0.51175934\n",
      "[Episode  1751]  steps:    10\n",
      "Loss:  10.173046\n",
      "[Episode  1801]  steps:    10\n",
      "Loss:  5.476295\n",
      "[Episode  1851]  steps:    12\n",
      "Loss:  10.28707\n",
      "[Episode  1901]  steps:    29\n",
      "Loss:  0.5149778\n",
      "[Episode  1951]  steps:    10\n",
      "Loss:  15.005966\n",
      "[Episode  2001]  steps:    17\n",
      "Loss:  0.5732801\n",
      "[Episode  2051]  steps:    23\n",
      "Loss:  0.62984455\n",
      "[Episode  2101]  steps:    15\n",
      "Loss:  0.58635813\n",
      "[Episode  2151]  steps:    11\n",
      "Loss:  5.658734\n",
      "[Episode  2201]  steps:    16\n",
      "Loss:  0.65263015\n",
      "[Episode  2251]  steps:    15\n",
      "Loss:  5.3123007\n",
      "[Episode  2301]  steps:    23\n",
      "Loss:  0.6129243\n",
      "[Episode  2351]  steps:    32\n",
      "Loss:  5.4924703\n",
      "[Episode  2401]  steps:    13\n",
      "Loss:  0.64433634\n",
      "[Episode  2451]  steps:    10\n",
      "Loss:  5.3498063\n",
      "[Episode  2501]  steps:    23\n",
      "Loss:  5.185662\n",
      "[Episode  2551]  steps:    26\n",
      "Loss:  19.601849\n",
      "[Episode  2601]  steps:    15\n",
      "Loss:  10.479002\n",
      "[Episode  2651]  steps:    31\n",
      "Loss:  0.6083507\n",
      "[Episode  2701]  steps:    22\n",
      "Loss:  0.7332606\n",
      "[Episode  2751]  steps:    16\n",
      "Loss:  10.031207\n",
      "[Episode  2801]  steps:    18\n",
      "Loss:  5.6864505\n",
      "[Episode  2851]  steps:    20\n",
      "Loss:  0.6976047\n",
      "[Episode  2901]  steps:    22\n",
      "Loss:  0.6707054\n",
      "[Episode  2951]  steps:    36\n",
      "Loss:  10.417406\n",
      "[Episode  3001]  steps:    25\n",
      "Loss:  1.0100304\n",
      "[Episode  3051]  steps:    21\n",
      "Loss:  0.6310217\n",
      "[Episode  3101]  steps:    16\n",
      "Loss:  5.328599\n",
      "[Episode  3151]  steps:    17\n",
      "Loss:  10.207234\n",
      "[Episode  3201]  steps:    22\n",
      "Loss:  5.829217\n",
      "[Episode  3251]  steps:    26\n",
      "Loss:  0.9338707\n",
      "[Episode  3301]  steps:    24\n",
      "Loss:  9.7003765\n",
      "[Episode  3351]  steps:    28\n",
      "Loss:  0.7476536\n",
      "[Episode  3401]  steps:    27\n",
      "Loss:  0.5601077\n",
      "[Episode  3451]  steps:    20\n",
      "Loss:  0.8510299\n",
      "[Episode  3501]  steps:    26\n",
      "Loss:  0.53713655\n",
      "[Episode  3551]  steps:    21\n",
      "Loss:  0.6758433\n",
      "[Episode  3601]  steps:    17\n",
      "Loss:  6.031184\n",
      "[Episode  3651]  steps:    24\n",
      "Loss:  0.68043065\n",
      "[Episode  3701]  steps:    27\n",
      "Loss:  10.513034\n",
      "[Episode  3751]  steps:    42\n",
      "Loss:  0.84713155\n",
      "[Episode  3801]  steps:    21\n",
      "Loss:  0.53365695\n",
      "[Episode  3851]  steps:    28\n",
      "Loss:  0.94193095\n",
      "[Episode  3901]  steps:    28\n",
      "Loss:  0.6392381\n",
      "[Episode  3951]  steps:    28\n",
      "Loss:  0.5169442\n",
      "[Episode  4001]  steps:    29\n",
      "Loss:  0.47619992\n",
      "[Episode  4051]  steps:    26\n",
      "Loss:  0.50543994\n",
      "[Episode  4101]  steps:    26\n",
      "Loss:  0.47990862\n",
      "[Episode  4151]  steps:    30\n",
      "Loss:  5.57121\n",
      "[Episode  4201]  steps:    17\n",
      "Loss:  5.3096642\n",
      "[Episode  4251]  steps:    47\n",
      "Loss:  0.5092739\n",
      "[Episode  4301]  steps:    38\n",
      "Loss:  0.48521835\n",
      "[Episode  4351]  steps:    41\n",
      "Loss:  5.502726\n",
      "[Episode  4401]  steps:    15\n",
      "Loss:  0.48610973\n",
      "[Episode  4451]  steps:    38\n",
      "Loss:  0.4986121\n",
      "[Episode  4501]  steps:    18\n",
      "Loss:  0.49225792\n",
      "[Episode  4551]  steps:    15\n",
      "Loss:  5.5486517\n",
      "[Episode  4601]  steps:    24\n",
      "Loss:  0.5187607\n",
      "[Episode  4651]  steps:    22\n",
      "Loss:  0.53109986\n",
      "[Episode  4701]  steps:    38\n",
      "Loss:  0.49636745\n",
      "[Episode  4751]  steps:    22\n",
      "Loss:  5.4066806\n",
      "[Episode  4801]  steps:    24\n",
      "Loss:  0.5734064\n",
      "[Episode  4851]  steps:    19\n",
      "Loss:  10.495657\n",
      "[Episode  4901]  steps:    42\n",
      "Loss:  5.4833035\n",
      "[Episode  4951]  steps:    21\n",
      "Loss:  5.4057703\n",
      "Total score: 18.0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마치며..\n",
    "1. 매번 할때마다 잘학습할때도 있고 10근처에서 계속 놀아날때도 있다\n",
    "   initializer 로 xavier 안쓰니까 그런거같다.\\\n",
    "   -라고 생각했지만 glorot 방식으로 다시 코딩해도 비슷한결과다. 현상은 초기 initialization 이 문젠거같은데..\\\n",
    "   -라고 생각했지만 역시 2013년 버전이라서 non-stationary target 으로 인한 Q 가 Diverge 하는 이유가 큰거같다.\\\n",
    "      -뭔가 수렴하는데 필요한 episode 가 굉장히 천차만별인 느낌.\n",
    "   \n",
    "2. rendering 이 너무 빨리끝나버려서 잘 못본다. 마지막 결과를 반복적으로 보는 기능을 찾고싶다.\n",
    "3. tensorflow 2.0 이 1.0을 포함하기도하지만 완전 리메이크버전인느낌도 든다.. 공부해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
