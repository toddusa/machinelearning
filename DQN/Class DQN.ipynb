{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Class\n",
    "\n",
    "DQN(NIPS-2013)\n",
    "\"Playing Atari with Deep Reinforcement Learning\"\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "\n",
    "DQN(Nature-2015)\n",
    "\"Human-level control through deep reinforcement learning\"\n",
    "http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://github.com/hunkim/ReinforcementZeroToAll/blob/master/dqn.py\n",
    "\n",
    "# DQN class can\n",
    "\n",
    "1) Build network\\\n",
    "2) Predict Q_value given state\\\n",
    "3) Train parameters\n",
    "\n",
    "Args :\n",
    "\n",
    " session (tf.Session): Tensorflow session\\\n",
    " input_size (int): Input dimension\\\n",
    " output_size (int): Number of discrete actions\\\n",
    " name (str, optional): TF Graph will be built under this name scope\n",
    " \n",
    "# DQN Network architecture (simple MLP) :\n",
    "\n",
    "Args:\n",
    "\n",
    " h_size (int, optional): Hidden layer dimension\\\n",
    " l_rate (float, optional): Learning rate\n",
    " \n",
    "acknowledgement:\n",
    "\n",
    "hidden layer 수 = 1\\\n",
    "hidden layer dimension/node 수 =10\\\n",
    "learning rate = 0.001\\\n",
    "activation function = reLu\\\n",
    "loss function = MSE\\\n",
    "optimizer = adam\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, session: tf.compat.v1.Session, input_size: int, output_size: int, name: str=\"main\"):\n",
    "        \n",
    "        # tensorflow 2.0.0버전에선 tf.Session 이사용되지않음.  tf.compat.v1.Session 로 변경\n",
    "        \n",
    "        self.session =session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        self._build_network()\n",
    "        \n",
    "#===============================================================================================\n",
    "        \n",
    "    def _build_network(self, h_size = 10, l_rate = 0.001):\n",
    "    \n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            net = self._X\n",
    "\n",
    "            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n",
    "            net = tf.layers.dense(net, self.output_size)\n",
    "            self._Qpred = net\n",
    "\n",
    "            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n",
    "            \n",
    "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
    "            self._train = optimizer.minimize(self._loss)\n",
    "            \n",
    "#===============================================================================================\n",
    "            \n",
    "    # Q(s,a) 을 리턴 하는 predict 함수\n",
    "    # state (np.ndarray): State array, shape (n, imput_dim)\n",
    "    # return np.ndarray: Q value array, shape (n, output_dim)\n",
    "    \n",
    "    \n",
    "    def predict(self, state: np.ndarray):\n",
    "        \n",
    "        x = np.reshape(state, [-1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "    \n",
    "#===============================================================================================\n",
    "    # X, y 를 받아 업데이트 하고 loss, result from train step 을 리턴\n",
    "    \n",
    "    \n",
    "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray):\n",
    "       \n",
    "        feed = {\n",
    "            self._X: x_stack,\n",
    "            self._Y: y_stack\n",
    "        }\n",
    "        return self.session.run([self._loss, self._train], feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

