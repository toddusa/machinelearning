{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Ai Gym cartpole 을 nature 2015  방법으로 DQN 학습!\n",
    "\n",
    "\"https://www.nature.com/articles/nature14236\"\n",
    "\n",
    "Nature 2015 keypoint:\n",
    "\n",
    "기존에 \n",
    "\n",
    "1. Correlations between samples\n",
    "2. Non stationary target \n",
    "\n",
    "위와같은 이유로 Q 수렴성이 떨어지는 문제를\n",
    "\n",
    "1. Go deep *2013\n",
    "2. Capture and replay *2013\n",
    "3. Separate networks *2015\n",
    "\n",
    "의 해결방법으로 해결함\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\toddu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "#이거 확실히 v2 랑 v1 공부해야할듯\n",
    "# https://www.youtube.com/watch?v=FvAsdZwavpA\n",
    "# tensorflow2.0 이 좀더 직관적이고 hihg level API 인 keras 위주로 가는것같음\n",
    "# https://www.tensorflow.org/guide/migrate 이민가이드고.. 단하나 contrib 는 예욐ㅋㅋ\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN AI 의 Cartpole 환경 도입\n",
    "\n",
    "1. input_size = state 가 어떤걸 의미하는지 알수없는 4개의 숫자로 표현이됨. env.step(action)의 첫행임\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "\n",
    " <p>\n",
    "action = env.action_space.sample()\n",
    "    \n",
    "observation, reward, done, _ = env.step(action)\n",
    " </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "2. output_size = 왼쪽 혹은 오른쪽 2개\n",
    "3. discount factor = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env =gym.make('CartPole-v0')\n",
    "#env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size= env.action_space.n\n",
    "\n",
    "env._max_episode_steps = 10001\n",
    "\n",
    "# constants difining our neural network\n",
    "\n",
    "dis =0.9\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 10\n",
    "sigma = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN class 정의\n",
    "\n",
    "ref: https://github.com/hunkim/ReinforcementZeroToAll/blob/master/dqn.py 에서 수정\n",
    "\n",
    "# DQN class can\n",
    "\n",
    "1) Build network\\\n",
    "2) Predict Q_value given state\\\n",
    "3) Train parameters\n",
    "\n",
    "Args :\n",
    "\n",
    " session (tf.Session): Tensorflow session\\\n",
    " input_size (int): Input dimension\\\n",
    " output_size (int): Number of discrete actions\\\n",
    " name (str, optional): TF Graph will be built under this name scope\n",
    " \n",
    "# DQN Network architecture :\n",
    "\n",
    "Args:\n",
    "\n",
    " h_size (int, optional): Hidden layer dimension\\\n",
    " l_rate (float, optional): Learning rate\n",
    " \n",
    "acknowledgement:\n",
    "\n",
    "hidden layer 수 = 1\\\n",
    "hidden layer dimension/node 수 =10\\\n",
    "learning rate = 0.001\\\n",
    "activation function = tanh\\\n",
    "loss function = MSE\\\n",
    "optimizer = adam\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\"):\n",
    "        \n",
    "        # tensorflow 2.0.0버전에선 tf.Session 이사용되지않음.  tf.compat.v1.Session 로 변경 했다가 위에 import 를 아예 바꿔버림\n",
    "        \n",
    "        self.session =session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        self._build_network()\n",
    "        \n",
    "#===============================================================================================\n",
    "        \n",
    "    def _build_network(self, h_size = 10, l_rate = 0.1):\n",
    "    \n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder( shape=[None, self.input_size], name=\"input_x\", dtype=tf.float32)\n",
    "            \n",
    "            #first layer \n",
    "            W1= tf.get_variable(\"W1\", shape=[self.input_size, h_size], \n",
    "                                initializer=tf.keras.initializers.glorot_uniform())\n",
    "            \n",
    "            #initializer=tf.contrib.layers.xavier_initializer()) 였지만\n",
    "            # 나도 contrib 문제를 겪게되어 아영이의 initializer로 바꾸어 주었다.\n",
    "            # tensorflow_core.compat.v1 조차도 contrib API가 없어져ㅆ다..\n",
    "            #로할라했지만 contrib 없이 xavier Glorot (사람이름) 의 다른방법 = Glorot 이 있어서 그걸로 하였다.\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform?version=stable\n",
    "            \n",
    "            layer1 = tf.nn.tanh(tf.matmul(self._X, W1))\n",
    "            \n",
    "            #Second layer             \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, self.output_size], \n",
    "                                 initializer=tf.keras.initializers.glorot_uniform())\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Q_prediction\n",
    "            self._Qpred = tf.matmul(layer1, W2)\n",
    "            \n",
    "            \n",
    "            #we need to define the parts of the network needed for learning a policy\n",
    "            self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "            \n",
    "            #loss function                   \n",
    "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
    "\n",
    "            #learning            \n",
    "            self._train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self._loss)\n",
    "           \n",
    "            \n",
    "#===============================================================================================\n",
    "            \n",
    "    # Q(s,a) 을 리턴 하는 predict 함수\n",
    "    # state (np.ndarray): State array, shape (n, imput_dim)\n",
    "    # return np.ndarray: Q value array, shape (n, output_dim)\n",
    "    \n",
    "    \n",
    "    def predict(self, state: np.ndarray):\n",
    "        \n",
    "        x = np.reshape(state, [1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "    \n",
    "#===============================================================================================\n",
    "    # X, y 를 받아 업데이트 하고 loss, result from train step 을 리턴\n",
    "    \n",
    "    \n",
    "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray):\n",
    "      \n",
    "        return self.session.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from replay buffer (Simple Replay Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_replay_train(DQN,train_batch):\n",
    "    x_stack = np.empty(0).reshape(0, DQN.input_size)\n",
    "    y_stack = np.empty(0).reshape(0, DQN.output_size)\n",
    "    \n",
    "    #get stored information from the buffer\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q=DQN.predict(state)\n",
    "        \n",
    "        #terminal?\n",
    "        if done:\n",
    "            Q[0,action] = reward\n",
    "        else:\n",
    "            #obtain Q' values by feeding the new state through our network\n",
    "            Q[0,action] =reward + dis*np.max(DQN.predict(next_state))\n",
    "            \n",
    "        x_stack = np.vstack([x_stack, state])    \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        \n",
    "    #train our network using target and predicted Q values on each episode\n",
    "    return DQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 네트워크를 받아 play 를 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN):\n",
    "    s=env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s, reward, done, _ =env.step(a)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            env.close()  #안하면 응답없음 걸려버림\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    \n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = DQN(sess, input_size, output_size)\n",
    "        \n",
    "        tf.global_variables_initializer().run()    # Weight 초기화\n",
    "        \n",
    "       \n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = 1. / ((episode/ 10)+1) #decaying epsilon\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            step_count = 0\n",
    "            \n",
    "                    \n",
    "            while not done:\n",
    "                \n",
    "                #epillon 의 확률로 random action 을하고 나머지 확률로 argmax\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample() \n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "                    \n",
    "                #get new state, reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                #끝나지않는게 목적이므로 패널티\n",
    "                if done:\n",
    "                    reward = -100\n",
    "\n",
    "                    \n",
    "                #경험을 replay_buffer 에 저장. 위의 deque 에서 replay memory 크기를 50000으로 한정\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                \n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "                \n",
    "                       \n",
    "                \n",
    "            if episode % 10 ==1:\n",
    "                for _ in range(50):\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ =simple_replay_train(mainDQN, minibatch)\n",
    "                \n",
    "                    \n",
    "            if episode % 5 ==1:\n",
    "                print(\"[Episode {:>5}]  steps: {:>5} Loss: {:4f} \".format(episode, step_count, loss))\n",
    "                #print(\"Loss: \", loss)\n",
    "            \n",
    "            last_100_game_reward.append(step_count)\n",
    "            #print(last_100_game_reward)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward >3000:\n",
    "                    print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "          \n",
    "        \n",
    "        bot_play(mainDQN)\n",
    "        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     1]  steps:    22 Loss: 633.896790 \n",
      "[Episode     6]  steps:     9 Loss: 633.896790 \n",
      "[Episode    11]  steps:    11 Loss: 353.407898 \n",
      "[Episode    16]  steps:    58 Loss: 353.407898 \n",
      "[Episode    21]  steps:    67 Loss: 6.786071 \n",
      "[Episode    26]  steps:   101 Loss: 6.786071 \n",
      "[Episode    31]  steps:   155 Loss: 2.982317 \n",
      "[Episode    36]  steps:    82 Loss: 2.982317 \n",
      "[Episode    41]  steps:    64 Loss: 576.675659 \n",
      "[Episode    46]  steps:   110 Loss: 576.675659 \n",
      "[Episode    51]  steps:   113 Loss: 2.328657 \n",
      "[Episode    56]  steps:    45 Loss: 2.328657 \n",
      "[Episode    61]  steps:    44 Loss: 1.510682 \n",
      "[Episode    66]  steps:    58 Loss: 1.510682 \n",
      "[Episode    71]  steps:    13 Loss: 1.428538 \n",
      "[Episode    76]  steps:    57 Loss: 1.428538 \n",
      "[Episode    81]  steps:    57 Loss: 0.911943 \n",
      "[Episode    86]  steps:     9 Loss: 0.911943 \n",
      "[Episode    91]  steps:     9 Loss: 544.165833 \n",
      "[Episode    96]  steps:    84 Loss: 544.165833 \n",
      "[Episode   101]  steps:    54 Loss: 2.439790 \n",
      "[Episode   106]  steps:    85 Loss: 2.439790 \n",
      "[Episode   111]  steps:    87 Loss: 1.404019 \n",
      "[Episode   116]  steps:    19 Loss: 1.404019 \n",
      "[Episode   121]  steps:    29 Loss: 3.093938 \n",
      "[Episode   126]  steps:     9 Loss: 3.093938 \n",
      "[Episode   131]  steps:     9 Loss: 2.920323 \n",
      "[Episode   136]  steps:    11 Loss: 2.920323 \n",
      "[Episode   141]  steps:     8 Loss: 1.046178 \n",
      "[Episode   146]  steps:     9 Loss: 1.046178 \n",
      "[Episode   151]  steps:    11 Loss: 2.179631 \n",
      "[Episode   156]  steps:    24 Loss: 2.179631 \n",
      "[Episode   161]  steps:    29 Loss: 500.474426 \n",
      "[Episode   166]  steps:    15 Loss: 500.474426 \n",
      "[Episode   171]  steps:    21 Loss: 552.097717 \n",
      "[Episode   176]  steps:   119 Loss: 552.097717 \n",
      "[Episode   181]  steps:   115 Loss: 5.241382 \n",
      "[Episode   186]  steps:    42 Loss: 5.241382 \n",
      "[Episode   191]  steps:    97 Loss: 537.841919 \n",
      "[Episode   196]  steps:    33 Loss: 537.841919 \n",
      "[Episode   201]  steps:    52 Loss: 9.972127 \n",
      "[Episode   206]  steps:   112 Loss: 9.972127 \n",
      "[Episode   211]  steps:    83 Loss: 2.372216 \n",
      "[Episode   216]  steps:    93 Loss: 2.372216 \n",
      "[Episode   221]  steps:   207 Loss: 2.260144 \n",
      "[Episode   226]  steps:    40 Loss: 2.260144 \n",
      "[Episode   231]  steps:    99 Loss: 2.866929 \n",
      "[Episode   236]  steps:     8 Loss: 2.866929 \n",
      "[Episode   241]  steps:    10 Loss: 2.892649 \n",
      "[Episode   246]  steps:    12 Loss: 2.892649 \n",
      "[Episode   251]  steps:     8 Loss: 1.155773 \n",
      "[Episode   256]  steps:    36 Loss: 1.155773 \n",
      "[Episode   261]  steps:    33 Loss: 9.963412 \n",
      "[Episode   266]  steps:     8 Loss: 9.963412 \n",
      "[Episode   271]  steps:    10 Loss: 5.036565 \n",
      "[Episode   276]  steps:     9 Loss: 5.036565 \n",
      "[Episode   281]  steps:     9 Loss: 4.954385 \n",
      "[Episode   286]  steps:    56 Loss: 4.954385 \n",
      "[Episode   291]  steps:    50 Loss: 6.020750 \n",
      "[Episode   296]  steps:    44 Loss: 6.020750 \n",
      "[Episode   301]  steps:    50 Loss: 1.873467 \n",
      "[Episode   306]  steps:    26 Loss: 1.873467 \n",
      "[Episode   311]  steps:    40 Loss: 1.681848 \n",
      "[Episode   316]  steps:    22 Loss: 1.681848 \n",
      "[Episode   321]  steps:    14 Loss: 3.580076 \n",
      "[Episode   326]  steps:    57 Loss: 3.580076 \n",
      "[Episode   331]  steps:    90 Loss: 5.404405 \n",
      "[Episode   336]  steps:     9 Loss: 5.404405 \n",
      "[Episode   341]  steps:     9 Loss: 5.530284 \n",
      "[Episode   346]  steps:    64 Loss: 5.530284 \n",
      "[Episode   351]  steps:    41 Loss: 2.480109 \n",
      "[Episode   356]  steps:   701 Loss: 2.480109 \n",
      "[Episode   361]  steps:    49 Loss: 1.066559 \n",
      "[Episode   366]  steps:    67 Loss: 1.066559 \n",
      "[Episode   371]  steps:   284 Loss: 1.425947 \n",
      "[Episode   376]  steps:  3811 Loss: 1.425947 \n",
      "[Episode   381]  steps:  2538 Loss: 0.767933 \n",
      "[Episode   386]  steps: 10001 Loss: 0.767933 \n",
      "[Episode   391]  steps:  1053 Loss: 1.372256 \n",
      "[Episode   396]  steps:  1354 Loss: 1.372256 \n",
      "[Episode   401]  steps:   178 Loss: 0.639033 \n",
      "[Episode   406]  steps:   134 Loss: 0.639033 \n",
      "[Episode   411]  steps:  5788 Loss: 0.740578 \n",
      "[Episode   416]  steps:   245 Loss: 0.740578 \n",
      "[Episode   421]  steps:   114 Loss: 0.896966 \n",
      "[Episode   426]  steps:   437 Loss: 0.896966 \n",
      "[Episode   431]  steps:    69 Loss: 0.585283 \n",
      "[Episode   436]  steps:   121 Loss: 0.585283 \n",
      "[Episode   441]  steps:   143 Loss: 1.207983 \n",
      "[Episode   446]  steps:   132 Loss: 1.207983 \n",
      "[Episode   451]  steps:   176 Loss: 1.032438 \n",
      "[Episode   456]  steps:  1478 Loss: 1.032438 \n",
      "[Episode   461]  steps:   628 Loss: 0.842109 \n",
      "[Episode   466]  steps:   692 Loss: 0.842109 \n",
      "[Episode   471]  steps:   245 Loss: 1.669010 \n",
      "[Episode   476]  steps:   218 Loss: 1.669010 \n",
      "[Episode   481]  steps:   771 Loss: 1.606710 \n",
      "[Episode   486]  steps:  2004 Loss: 1.606710 \n",
      "[Episode   491]  steps:  2253 Loss: 1.821231 \n",
      "[Episode   496]  steps:   852 Loss: 1.821231 \n",
      "[Episode   501]  steps:   638 Loss: 0.974873 \n",
      "[Episode   506]  steps:  1031 Loss: 0.974873 \n",
      "[Episode   511]  steps: 10001 Loss: 1.481009 \n",
      "[Episode   516]  steps: 10001 Loss: 1.481009 \n",
      "[Episode   521]  steps: 10001 Loss: 1.529128 \n",
      "[Episode   526]  steps: 10001 Loss: 1.529128 \n",
      "[Episode   531]  steps: 10001 Loss: 1.120721 \n",
      "Game Cleared in 534 episodes with avg reward 3040.21\n",
      "Total score: 10001.0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마치며..\n",
    "1. 매번 할때마다 잘학습할때도 있고 10근처에서 계속 놀아날때도 있다\n",
    "   initializer 로 xavier 안쓰니까 그런거같다.\\\n",
    "   - 라고 생각했지만 glorot 방식으로 다시 코딩해도 비슷한결과다. 현상은 초기 initialization 이 문젠거같은데..\n",
    "   - 라고 생각했지만 역시 2013년 버전이라서 non-stationary target 으로 인한 Q 가 Diverge 하는 이유가 큰거같다.\n",
    "   - 뭔가 수렴하는데 필요한 episode 가 굉장히 천차만별인 느낌.\n",
    "   - main 함수에 미니배치 한번에 50번학습시키는 for _ in range(50): 만들었더니 더 잘학습한다.\n",
    "2. rendering 이 너무 빨리끝나버려서 잘 못본다. 마지막 결과를 반복적으로 보는 기능을 찾고싶다.\n",
    "3. tensorflow 2.0 이 1.0을 포함하기도하지만 완전 리메이크버전인느낌도 든다.. 공부해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
